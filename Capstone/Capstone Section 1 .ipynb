{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b7f9be3",
   "metadata": {},
   "source": [
    "Extract Data 1.1 \n",
    "\n",
    "For “Credit Card System,” create a Python and PySpark SQL program to read/extract the following JSON files according to the specifications found in the mapping document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "982b626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccessary Pyspark modules\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# Create or retrieve an existing SparkSession.\n",
    "# This is the entry point to use DataFrame and SQL functionality in PySpark.\n",
    "# The 'appName' is used to name your Spark application, which helps when monitoring jobs in Spark UI.\n",
    "spark = SparkSession.builder.appName('Credit Card Data Loader').getOrCreate()\n",
    "# Load the 'cdw_sapp_branch.json' JSON file into a DataFrame.\n",
    "# Load the 'cdw_sapp_credit.json' JSON file into a DataFrame.\n",
    "# Load the 'cdw_sapp_customer.json' JSON file into a DataFrame.\n",
    "# The 'option(\"multiline\", \"true\")' tells Spark to handle JSON entries that span multiple lines.\n",
    "# This is useful when each record is formatted across several lines for readability.\n",
    "df_branch = spark.read.option(\"multiline\", \"true\").json(r\"C:\\Users\\malik.alston\\Desktop\\Data\\Capstone\\Credit Card Dataset Overview\\cdw_sapp_branch.json\")\n",
    "df_cc = spark.read.option(\"multiline\", \"true\").json(r\"C:\\Users\\malik.alston\\Desktop\\Data\\Capstone\\Credit Card Dataset Overview\\cdw_sapp_credit.json\")\n",
    "df_customer = spark.read.option(\"multiline\", \"true\").json(r\"C:\\Users\\malik.alston\\Desktop\\Data\\Capstone\\Credit Card Dataset Overview\\cdw_sapp_customer.json\")\n",
    "# The below lines are currently commented out but can be used to visually inspect\n",
    "# The contents of each DataFrame. They display the top 20 rows by default in a tabular format.\n",
    "# df_customer.show()\n",
    "# df_branch.show()\n",
    "# df_cc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088517f3",
   "metadata": {},
   "source": [
    "Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import initcap, lower, concat_ws, format_string, col, lpad, lit, concat, when\n",
    "# initcap()\tCapitalize first letter of each word, lower() Convert string to lowercase, concat_ws()\tConcatenate with separator, format_string()\tString formatting\n",
    "# col()\tReference to a column, lpad() Left-pad a string, lit()\tAdd a constant/literal column, concat()\tConcatenate columns (no separator), when() Conditional logic\n",
    "df_customer = df_customer.withColumn(\"FIRST_NAME\", initcap(\"FIRST_NAME\"))\n",
    "df_customer = df_customer.withColumn(\"LAST_NAME\", initcap(\"LAST_NAME\"))\n",
    "df_customer = df_customer.withColumn(\"MIDDLE_NAME\", lower(\"MIDDLE_NAME\"))\n",
    "df_customer = df_customer.withColumn(\"ADDRESS\", concat_ws(\",\", \"STREET_NAME\", \"APT_NO\"))\n",
    "df_customer = df_customer.withColumn(\"CUST_PHONE\",concat(lit(\"(XXX)\"),col(\"CUST_PHONE\").substr(1, 3),lit(\"-\"),col(\"CUST_PHONE\").substr(4, 4)))\n",
    "\n",
    "df_branch = df_branch.withColumn(\"BRANCH_PHONE\",concat(lit(\"(XXX)\"),col(\"BRANCH_PHONE\").substr(1, 3),lit(\"-\"),col(\"BRANCH_PHONE\").substr(4, 4)))\n",
    "df_branch = df_branch.withColumn(\"BRANCH_ZIP\", when(col(\"BRANCH_ZIP\").isNull(), lit(\"99999\")).otherwise(col(\"BRANCH_ZIP\")))\n",
    "\n",
    "df_cc = df_cc.withColumn(\"TIMEID\", format_string(\"%04d%02d%02d\", col(\"YEAR\"), col(\"MONTH\"), col(\"DAY\")))\n",
    "# The below lines are currently commented out but can be used to visually inspect\n",
    "# The contents of each DataFrame. They display the top 20 rows by default in a tabular format.\n",
    "#df_customer.show()\n",
    "#df_branch.show()\n",
    "#df_cc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650b4457",
   "metadata": {},
   "source": [
    "Load Data 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261fca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DataFrames transferred to MySQL.\n"
     ]
    }
   ],
   "source": [
    "# Define the JDBC MySQL connection URL\n",
    "mysql_url = \"jdbc:mysql://localhost:3306/creditcard_capstone\"\n",
    "# Define the configuration dictionary for MySQL connection\n",
    "mysql_config = {\n",
    "    \"user\": \"root\",   # MySQL username\n",
    "    \"password\": \"password\",   # MySQL password\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"   # JDBC driver for MySQL\n",
    "}\n",
    "# Write the DataFrame 'df_customer' to the MySQL table 'CDW_SAPP_CUSTOMER'\n",
    "# 'mode=\"append\"' ensures that data will be added to the existing table without overwriting it\n",
    "df_customer.write.jdbc(\n",
    "    url=mysql_url,   # JDBC connection URL\n",
    "    table=\"CDW_SAPP_CUSTOMER\",   # Destination table name in MySQL\n",
    "    mode=\"append\",  # use 'append' if you want to preserve existing data\n",
    "    properties=mysql_config   # Pass the MySQL configuration\n",
    ")\n",
    "# Write the DataFrame 'df_branch' to the MySQL table 'CDW_SAPP_BRANCH'\n",
    "# Again, 'append' mode is used to add data without deleting existing records\n",
    "df_branch.write.jdbc(\n",
    "    url=mysql_url,\n",
    "    table=\"CDW_SAPP_BRANCH\",\n",
    "    mode=\"append\",\n",
    "    properties=mysql_config\n",
    ")\n",
    "# Write the DataFrame 'df_cc' to the MySQL table 'CDW_SAPP_CREDIT_CARD'\n",
    "# This transfers credit card transaction data to the specified table\n",
    "df_cc.write.jdbc(\n",
    "    url=mysql_url,\n",
    "    table=\"CDW_SAPP_CREDIT_CARD\",\n",
    "    mode=\"append\",\n",
    "    properties=mysql_config\n",
    ")\n",
    "# Confirmation message to indicate all DataFrames have been written to MySQL\n",
    "print(\"All DataFrames transferred to MySQL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbc02ce",
   "metadata": {},
   "source": [
    "df_customer.printSchema()\n",
    "df_branch.printSchema()\n",
    "df_cc.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16097aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2451fc8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee29542d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
